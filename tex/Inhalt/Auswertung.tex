\chapter{Fazit und Ausblick}

% Blick in Bachelorarbeit

% Zusammenfassung
- Ziel war es ...

% Theoretische Grundlagen
- Nach der Diskussion verwandter Arbeiten in Kapitel x wurden zunächst die für die Arbeit relevanten graphentheoretischen Begriffe definiert.
- graphentheoretische Grundlagen (Definitionen)
- Anwendungszenarien / Arten von Netzwerken -> Schwerpunkt auf Informations- und Wissensnetzwerke
- Die Betrachtung verschiedener Netzwerkarten hat verdeutlicht, dass es graphenbasierte Softwaresysteme mit verschiedenen Schwerpunkten geben muss ...

- Top Down Prinzip!: graphenbasierte Softwaresysteme -> Graphdatenbanksystemen -> Vorauswahl -> funktionaler Vergleich -> Auswahl -> Benchmark

- Kategorisierung: Graphdatenbanksysteme, Graph Processing Systems, Analyse- und Visualisierungssoftware
	- weiter auf GDBMS eingegangen -> Definition und weitere Unterteilung
		- Def: OLTP-artig (Mehrbenutzerfähigkeit, transaktionale Performance, Integrität, Verfügbarkeit)
			- nativ / nicht-nativ
			- zentral / verteilt
			- eingebettet / Client-Server
			- disk- / hauptspeicherzentriert
			
- Datenmodelle (betrachtet: PGM, PHGM, RDF)
	- PGM: gerichteter, kantenbezeichneter, attributierter Multigraph
		- flexibel -> für viele Anwendungsfälle einsetzbar
		- Verantwortung bzgl. Schemaverwaltung an Anwendung übergeben
		- semistrukturierte Daten (Schemaevolution) -> Integration heterogener Datenquellen
	- PHGM: Erweiterung um n-äre Beziehungen .. seltenes, sehr spezielles Datenmodell
	- RDF in Verbindung mit SPARQL:
		- Schwerpunkte Inferenz und virtuelles Integrieren entsprechen nicht den Schwerpunkten des Forschungsvorhabens
		- fehlende modellinhärente Differenzierung in eine Beziehung zwischen Ressourcen und einer Beschreibung einzelner Ressourcen zwischen Attribut und Beziehung erschwert das Formulieren analytischer Anfragen
		- keine dynamischen Pfadinstanzen
	-> PGM und (P)HGM für das Forschungsvorhaben geeignet
	
- Operationen:
		

% Funktionaler Vergleich

% Benchmark
- hat ergeben, dass

% Fazit
- Verbinden der Ergebnisse aus funktionaler Evaluation und Benchmark


% Ausblick

% generell
- standardisierte Anfragesprache fehlt weiterhin
	- Gremlin/Blueprints bildet Quasi-Standard
		- Anpassung der GDBMS-Wrapper ist Community-driven (verzögert)
	- keine Bestrebungen hinsichtlich Standardisierung erkennbar
	- Cypher ist guter Kandidat für Standard
- Eignung Graph Processing für Forschungsvorhaben
	
- Rückwirkung auf Dokumentationen (OrientDB Transaktionen, Titan Kardinalität. Neo4j Anfragebeispiele)


% Ergebnis / Fazit


- Neo4j
	- Verwendung von Cypher
		- bei Performance-Problemen (Erreichbarkeit)... Nutzung der nativen API
		- Einbettung von Cypher im Quellcode -> hoher Wartungsaufwand (Holzschuher)
		- einfaches, intuitives Formulieren graphenbasierter Anfragen
- Titan

- beide
	- analytische Anfragen möglich (Cypher, Gremlin)
	- Gremlin
		- höherer Aufwand bei der Anfrageformulierung



% Benchmark
- Import: lesen und Schreiben von einer Platte (unrealistisch)
- parallele Lesezugriffe
- warmup-Prozedur verhindert Feststellen der Leistungsfähigkeit des Speichersystems
	-> weiterführende Benchmarks
- Betrachtung ohne Caches sinnvoll?
	
% Ausblick


- Neo4j:
	- In-Memory-Erweiterungen für Analyse
	- Query-Optimierung
	- formale Untersuchung und Standardisierung von Cypher im GDBMS-Sektor
	- ausführlicher Benchmark (parallele Queries, reale Unternehmensdaten)
	
- OrientDB
	- neues Speichersystem evaluieren
- HyperGraphDB
	- Implementierung Blueprints API
	
- quelloffene hauptspeicher-zentrierte Implementierung


